{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "3d8cd29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from scipy.stats import t\n",
    "from scipy.ndimage.filters import uniform_filter1d\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.feature_selection import RFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5ab8938",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_pr = pd.read_pickle(\"./log_price.df\")\n",
    "volu = pd.read_pickle(\"./volume_usd.df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "c889d58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "volu = volu/1e10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a854cc87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# moving averages\n",
    "def mov_avg(A, B, window_length=60):\n",
    "    # Input: two 1440 x 1 numpy arrays\n",
    "    # Output: two 288 x 1 numpy arrays\n",
    "    \n",
    "    pr_avg = uniform_filter1d(A, size=window_length, mode='nearest')[::30]\n",
    "    vo_avg = uniform_filter1d(B, size=window_length, mode='nearest')[::30]\n",
    "\n",
    "    return pr_avg[-10:], vo_avg[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61a1747f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def MACD(A):\n",
    "#     # Input: 1440 x 1 DataFrame\n",
    "#     # Output: two 1440 x 1 numpy array\n",
    "#     df = np.exp(A)\n",
    "        \n",
    "#     # Get the 26-day EMA of the closing price\n",
    "#     k = df.ewm(span=26, adjust=False, min_periods=26).mean()\n",
    "#     # Get the 12-day EMA of the closing price\n",
    "#     d = df.ewm(span=12, adjust=False, min_periods=12).mean()\n",
    "\n",
    "#     # Subtract the 26-day EMA from the 12-Day EMA to get the MACD\n",
    "#     macd = k - d\n",
    "\n",
    "#     # Get the 9-Day EMA of the MACD for the Trigger line\n",
    "#     macd_s = np.array(macd.ewm(span=9, adjust=False, min_periods=9).mean())\n",
    "\n",
    "#     # Calculate the difference between the MACD - Trigger for the Convergence/Divergence value\n",
    "#     #macd_h = macd - macd_s\n",
    "    \n",
    "#     return np.array(macd), macd_s #, macd_h\n",
    "def MACD(A):\n",
    "    # Input: 1440 x 1 DataFrame\n",
    "    # Output: two 1440 x 1 numpy array\n",
    "    A = np.exp(A)\n",
    "        \n",
    "    # Get the 26-day EMA of the closing price\n",
    "    k = np_ewma_vectorized(A, 26)\n",
    "    # Get the 12-day EMA of the closing price\n",
    "    d = np_ewma_vectorized(A, 12)\n",
    "\n",
    "    # Subtract the 26-day EMA from the 12-Day EMA to get the MACD\n",
    "    macd = k - d\n",
    "\n",
    "    #Get the 9-Day EMA of the MACD for the Trigger line\n",
    "    macd_s = np.array(np_ewma_vectorized(macd, 9))\n",
    "\n",
    "    # Calculate the difference between the MACD - Trigger for the Convergence/Divergence value\n",
    "    #macd_h = macd - macd_s\n",
    "    \n",
    "    return k[-10:], d[-10:], np.array(macd)[-10:]#, macd_s#, macd_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "216783a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def np_ewma_vectorized(data, window, method = \"WMS\"):\n",
    "\n",
    "    if method == \"WMS\":\n",
    "        alpha = 1 / window\n",
    "    elif method == \"EMA\":\n",
    "        alpha = 2 / (window + 1.0)\n",
    "    alpha_rev = 1 - alpha\n",
    "    n = data.shape[0]\n",
    "\n",
    "    pows = alpha_rev**(np.arange(n+1))\n",
    "\n",
    "    scale_arr = 1 / pows[:-1]\n",
    "    offset = data[0] * pows[1:]\n",
    "    pw0 = alpha * alpha_rev**(n-1)\n",
    "\n",
    "    mult = data * pw0 * scale_arr\n",
    "    cumsums = mult.cumsum()\n",
    "    out = offset + cumsums * scale_arr[::-1]\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ff9ad87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RSI_np(A, window_length=14, method=\"WMS\"):\n",
    "    \"\"\"\n",
    "    Calculate RSI\n",
    "    A: numpy array of log price\n",
    "    method : \"SMA\": simple moving average,\n",
    "            \"WMS\": Wilder Smoothing Method,\n",
    "            \"EMA\": exponential moving average\n",
    "    \n",
    "    Return RSI for last three periods\n",
    "    \"\"\"\n",
    "    # transform log-price to price\n",
    "    A = np.exp(A)\n",
    "    tmp = np.diff(A)\n",
    "\n",
    "    gain = np.clip(tmp, a_min = 0, a_max = None)\n",
    "    loss = np.abs(np.clip(tmp, a_min = None, a_max = 0))\n",
    "\n",
    "    if method == \"WMS\":\n",
    "        avg_gain = np_ewma_vectorized(gain, window_length)[-10:]\n",
    "        avg_loss = np_ewma_vectorized(loss, window_length)[-10:]\n",
    "    else:\n",
    "        avg_gain = np_ewma_vectorized(gain, window_length, method = \"EMA\")[-10:]\n",
    "        avg_loss = np_ewma_vectorized(loss, window_length, method = \"EMA\")[-10:]\n",
    "    \n",
    "    rs = avg_gain / avg_loss\n",
    "    rsi = 100 - (100 / (1 + rs))\n",
    "    return rsi[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d99c0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def volChanges(B, window_length=30):\n",
    "    vol0 = np.mean(B[-30:])\n",
    "    vol1 = np.mean(B[-60:-30])\n",
    "    vol2 = np.mean(B[-90:-60])\n",
    "    vol3 = np.mean(B[-120:-90])\n",
    "    vol4 = np.mean(B[-180:-120])\n",
    "    \n",
    "    return np.array([vol3-vol4, vol2-vol3, vol1-vol2, vol0-vol1, vol0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a8d9f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getVolRatios(vol, volWindow=[10, 30, 60]):  \n",
    "    # Input: 1440 x 1 numpy array\n",
    "    # the window here is in minutes\n",
    "    # Output: 3 dim numpy array\n",
    "    \n",
    "    return np.array([sum(vol[-win:]) for win in volWindow]) / sum(vol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "adbf3271",
   "metadata": {},
   "outputs": [],
   "source": [
    "def priceVolCor(A, B, time=[1440, 720, 360]):\n",
    "    # Input: two 1440 x 1 numpy arrays\n",
    "    # Output: 3 dim numpy array\n",
    "    \n",
    "    pv_cor = A\n",
    "    temp = np.hstack((A, B))\n",
    "    pv_cor = [np.corrcoef(temp[-t:]) for t in time]\n",
    "    return np.array(pv_cor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2903ead2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# z-score of log-price\n",
    "def zScorePr(A):\n",
    "    # Input: 1440 x 1 numpy array\n",
    "    # Output: 3 dim numpy array\n",
    "    # the moving average log-price of 30min, 1h, and 3h\n",
    "    \n",
    "    # moving averages of 30 minutes\n",
    "    pr_avg_0 = uniform_filter1d(A, size=30, mode='nearest')\n",
    "    \n",
    "    # 1 hour (60 minutes)\n",
    "    pr_avg_1 = uniform_filter1d(A, size=60, mode='nearest')\n",
    "    \n",
    "    # 2 hour (120 minutes)\n",
    "    pr_avg_2 = uniform_filter1d(A, size=120, mode='nearest')\n",
    "    \n",
    "    # 3 hours (180 minutes)\n",
    "    pr_avg_3 = uniform_filter1d(A, size=180, mode='nearest')\n",
    "    \n",
    "    z0 = (A[-1] - pr_avg_0[-1]) / np.std(pr_avg_0)\n",
    "    z1 = (A[-1] - pr_avg_1[-1]) / np.std(pr_avg_1)\n",
    "    z2 = (A[-1] - pr_avg_2[-1]) / np.std(pr_avg_2)\n",
    "    z3 = (A[-1] - pr_avg_3[-1]) / np.std(pr_avg_3)\n",
    "    return np.array([z0, z1, z2, z3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f9006524",
   "metadata": {},
   "outputs": [],
   "source": [
    "def neg30logr(A,window_length=10):\n",
    "    logreturn = -np.diff(A,30)\n",
    "    pr_avg = uniform_filter1d(logreturn, size=window_length, mode='nearest')[::30]\n",
    "    return pr_avg[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "83105f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(A, B):\n",
    "    m1, m2 = mov_avg(A, B) #10, 5\n",
    "    macd1, _, macd = MACD(A[::30]) #10,10\n",
    "    RSI1 = RSI_np(A[::30]) #5\n",
    "    RSI2 = RSI_np(A[::30],method=\"EMA\") #5\n",
    "    #print(len(m1), len(m2))\n",
    "    #print(len(macd1), len(macd))\n",
    "    #print(np.shape(RSI_np(A[::30])))\n",
    "    return np.hstack((\n",
    "        m1, m2, #10, 5\n",
    "        macd1, macd, #10,10\n",
    "        RSI1,RSI2, #5,5\n",
    "        volChanges(B), #5\n",
    "        getVolRatios(B), #3\n",
    "#         priceVolCor(A, B),\n",
    "#         zScorePr(A), \n",
    "        neg30logr(A) #5\n",
    "    )).reshape((1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "81c11847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1440 (1, 58)\n"
     ]
    }
   ],
   "source": [
    "f = get_features(log_pr.iloc[:1440,0], volu.iloc[:1440,0])\n",
    "p = np.shape(f)[1]\n",
    "print(len(log_pr.iloc[0:1440,0]), np.shape(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a3f2fea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# test_res = []\n",
    "# selected_rank2 = []\n",
    "\n",
    "# pred = []\n",
    "# for asset in range(10):\n",
    "# #     t0 = time.time()\n",
    "\n",
    "#     fs = get_features(log_pr.iloc[:1440, asset], volu.iloc[:1440, asset])\n",
    "#     y = log_pr.iloc[1440+29, asset] - log_pr.iloc[1440-1, asset]\n",
    "\n",
    "#     d = 10\n",
    "\n",
    "#     for t in range(1440*162 - 30)[d::d]: # compute the predictions every 10 minutes\n",
    "#         f = get_features(log_pr.iloc[t:(t+1440), asset], volu.iloc[t:(t+1440), asset])\n",
    "#         fs = np.vstack((fs, f))\n",
    "#         y = np.vstack((y, log_pr.iloc[t+1440+29, asset] - log_pr.iloc[t+1440-1, asset]))\n",
    "\n",
    "#     #t_used = time.time() - t0\n",
    "#     #print(t_used, np.shape(fs), np.shape(y))\n",
    "    \n",
    "#     ftest = get_features(log_pr.iloc[10:1450, asset], volu.iloc[10:1450, asset])\n",
    "#     ytest = log_pr.iloc[1450+29, asset] - log_pr.iloc[1450-1, asset]\n",
    "\n",
    "#     ytest = []\n",
    "#     for t in range(264960-1470)[1440*163 - 30::d]: # compute the predictions every 10 minutes\n",
    "#         f = get_features(log_pr.iloc[t:(t+1440), asset], volu.iloc[t:(t+1440), asset])\n",
    "#         ftest = np.vstack((ftest, f))\n",
    "#         ytest = np.append(ytest, log_pr.iloc[t+1440+29, asset] - log_pr.iloc[t+1440-1, asset])\n",
    "    \n",
    "#     #glmnet\n",
    "#     cv_model = glmnet.cv_glmnet(fs, y)\n",
    "#     #find optimal lambda value that minimizes test MSE\n",
    "#     lambda_cv = cv_model[-3]\n",
    "#     cv_model = glmnet.glmnet(fs, y, lambda_=lambda_cv)\n",
    "    \n",
    "# #     with open('./model_{}.pkl'.format(asset),'wb') as f:\n",
    "# #         pickle.dump(cv_model,f)\n",
    "\n",
    "#     pred = np.append(pred,robjects.r.predict(cv_model,newx=ftest)[:,-1])\n",
    "# np.corrcoef(pred[:,0], ytest[:,0])[0,1]\n",
    "# #     t_used = time.time() - t0\n",
    "# #     print(t_used, np.shape(ftest), np.shape(ytest))\n",
    "    \n",
    "# #     model = LinearRegression()\n",
    "# #     model.fit(fs, y)\n",
    "\n",
    "# #     pred = model.predict(ftest)\n",
    "    \n",
    "# #     test_res.append(np.corrcoef(pred[:,0], ytest[:,0])[0,1])\n",
    "# #     #with open('./model_{}.pkl'.format(asset),'wb') as f:\n",
    "# #     #    pickle.dump(model,f)\n",
    "    \n",
    "# #     select = RFE(model, n_features_to_select=40, step=1).fit(fs, y)\n",
    "# #     #selected.append([i for i in range(p) if select.support_[i]])\n",
    "# #     selected_rank2.append(select.ranking_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "71021015",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_res = []\n",
    "# selected_rank2 = []\n",
    "\n",
    "# asset = 2\n",
    "# t0 = time.time()\n",
    "\n",
    "# fs = get_features(log_pr.iloc[:1440, asset], volu.iloc[:1440, asset])\n",
    "# y = log_pr.iloc[1440+29, asset] - log_pr.iloc[1440-1, asset]\n",
    "\n",
    "# d = 10\n",
    "\n",
    "# for t in range(1440*162 - 30)[d::d]: # compute the predictions every 10 minutes\n",
    "#     f = get_features(log_pr.iloc[t:(t+1440), asset], volu.iloc[t:(t+1440), asset])\n",
    "#     fs = np.vstack((fs, f))\n",
    "#     y = np.vstack((y, log_pr.iloc[t+1440+30, asset] - log_pr.iloc[t+1440, asset]))\n",
    "\n",
    "# #t_used = time.time() - t0\n",
    "# #print(t_used, np.shape(fs), np.shape(y))\n",
    "\n",
    "# ftest = get_features(log_pr.iloc[10:1450, asset], volu.iloc[10:1450, asset])\n",
    "# ytest = log_pr.iloc[1450+30, asset] - log_pr.iloc[1450, asset]\n",
    "\n",
    "# d = 10\n",
    "\n",
    "# for t in range(264960-1470)[1440*163 - 30::d]: # compute the predictions every 10 minutes\n",
    "#     f = get_features(log_pr.iloc[t:(t+1440), asset], volu.iloc[t:(t+1440), asset])\n",
    "#     ftest = np.vstack((ftest, f))\n",
    "#     ytest = np.vstack((ytest, log_pr.iloc[t+1440+30, asset] - log_pr.iloc[t+1440, asset]))\n",
    "\n",
    "# t_used = time.time() - t0\n",
    "# print(t_used, np.shape(ftest), np.shape(ytest))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a8e7da63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #linear model\n",
    "# model = LinearRegression()\n",
    "# model.fit(fs, y)\n",
    "\n",
    "# pred = model.predict(ftest)\n",
    "# np.corrcoef(pred[:,0], ytest[:,0])[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "da70bd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import rpy2\n",
    "# from rpy2.robjects.packages import importr\n",
    "# import rpy2.robjects.packages as rpackages\n",
    "# import rpy2.robjects as robjects\n",
    "# glmnet = rpackages.importr('glmnet')\n",
    "# base = importr(\"base\")\n",
    "# from rpy2.robjects import pandas2ri\n",
    "\n",
    "# # Convert pandas.DataFrames to R dataframes automatically.\n",
    "# pandas2ri.activate()\n",
    "# #cv_model = glmnet.cv_glmnet(fs, y)\n",
    "# #find optimal lambda value that minimizes test MSE\n",
    "# #lambda_cv = cv_model[-3]\n",
    "# cv_model = glmnet.glmnet(fs, y)\n",
    "#                          #, lambda_=lambda_cv)\n",
    "# pred = robjects.r.predict(cv_model,newx=ftest)\n",
    "# [np.corrcoef(pred[:,i], ytest[:,0])[0,1] for i in range(100)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ca51f1",
   "metadata": {},
   "source": [
    "# Try Out Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aac20b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mystandardize(D):\n",
    "    S = np.std(D, axis=0, ddof=1)\n",
    "    M = np.mean(D, axis = 0)\n",
    "    D_norm = (D-M)/S\n",
    "    return [D_norm, M, S]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "983afad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "asset = 2\n",
    "fs = pd.read_pickle(\"feature\"+str(asset)+\".df\")\n",
    "y = pd.read_pickle(\"y\"+str(asset)+\".df\")\n",
    "vol_list = [10,11,12,13,14,45,46,47,48,49,53,54,55,56,57]\n",
    "fs.iloc[:,vol_list] /= 1e8\n",
    "fs_train = fs.iloc[:144*121-3,:]\n",
    "fs_test = fs.iloc[144*121-3:,:]\n",
    "y_train = y.iloc[:144*121-3,:]\n",
    "y_test = y.iloc[144*121-3:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "babae716",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shushuz/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "/Users/shushuz/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.254e+03, tolerance: 1.742e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/shushuz/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1e-06\n",
      "[-1.64121598e+00  8.18835279e+00 -1.23593671e+01  4.74823498e+00\n",
      "  1.51080711e+00 -4.12154906e+00 -3.81049242e+00  4.97850126e+00\n",
      "  1.18076638e+01 -9.03785747e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00 -1.19303412e-01\n",
      "  0.00000000e+00 -7.26770887e-03 -9.08767199e-03  0.00000000e+00\n",
      " -7.74369733e-03 -0.00000000e+00  0.00000000e+00 -0.00000000e+00\n",
      " -0.00000000e+00  8.22416053e+00 -2.70247171e+00  0.00000000e+00\n",
      "  1.42914642e+01 -2.24885163e+01  1.63951209e+00  0.00000000e+00\n",
      " -7.11086783e+00  0.00000000e+00  9.54299078e+00  1.59392051e-02\n",
      " -1.10343664e-02 -1.73749551e-02  3.44130426e-03  1.07533287e-02\n",
      " -9.26460966e-03  7.62305120e-03  7.95414765e-03 -4.27065317e-03\n",
      " -4.42038534e-03  0.00000000e+00 -0.00000000e+00  0.00000000e+00\n",
      " -0.00000000e+00  0.00000000e+00 -0.00000000e+00 -7.34558011e-01\n",
      "  1.81117437e-01  6.54490209e+00 -0.00000000e+00  7.56399719e+00\n",
      " -2.34147509e+00  0.00000000e+00]\n",
      "0.02732997408114489\n",
      "0.00025075000000000005\n",
      "[ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  4.77894100e-02\n",
      "  1.92666761e-01  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00 -1.31506368e-01\n",
      " -0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00\n",
      " -0.00000000e+00  8.14816819e-01  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  1.10025953e-02\n",
      " -1.30890805e-02 -2.78191495e-03  1.02867575e-02 -5.12646507e-03\n",
      " -6.99667997e-03  7.80322694e-03  3.60941568e-03 -5.07671127e-03\n",
      "  5.37909889e-04  0.00000000e+00 -0.00000000e+00  0.00000000e+00\n",
      " -0.00000000e+00  0.00000000e+00 -0.00000000e+00 -0.00000000e+00\n",
      " -0.00000000e+00  0.00000000e+00 -0.00000000e+00  0.00000000e+00\n",
      " -0.00000000e+00  0.00000000e+00]\n",
      "0.04315955423906765\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shushuz/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "Y_norm_train = pd.DataFrame(mystandardize((y_train-y_train.shift(3)).dropna())[0])\n",
    "for alpha in [1e-6,1e-5,1e-4,1e-3]:\n",
    "#np.linspace(1e-6, 1e-3, num=5):\n",
    "    model = ElasticNet(l1_ratio = 1,alpha=alpha, \n",
    "                              fit_intercept = True, normalize = False, \n",
    "                         tol=0.0000001, max_iter = 100000)\n",
    "    model.fit(fs_train.shift(3).dropna(), Y_norm_train)\n",
    "    print(alpha)\n",
    "#    print(model.alpha_)\n",
    "#    print(model.l1_ratio_)\n",
    "    print(model.coef_)\n",
    "\n",
    "    pred = model.predict(fs_test)\n",
    "    # np.corrcoef((pred-y_test.iloc[:,0].shift(3)).dropna(),\\\n",
    "    #             (y_test.iloc[:,0]-y_test.iloc[:,0].shift(3)).dropna())[0,1]\n",
    "    print(np.corrcoef(pd.DataFrame(pred).shift(3).dropna().iloc[:,0],\\\n",
    "                 (y_test-y_test.shift(3)).dropna().iloc[:,0])[0,1])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c428ab6",
   "metadata": {},
   "source": [
    "# Arima Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "99eafc48",
   "metadata": {},
   "outputs": [],
   "source": [
    "asset = 9\n",
    "fs = pd.read_pickle(\"feature\"+str(asset)+\".df\")\n",
    "y = pd.read_pickle(\"y\"+str(asset)+\".df\")\n",
    "fs_train = fs.iloc[:23325,:]\n",
    "fs_test = fs.iloc[23325:,:]\n",
    "y_train = y.iloc[:23325,:]\n",
    "y_test = y.iloc[23325:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "1a6bb0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = log_pr.iloc[29::30,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "e87fe6f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "from pmdarima.arima.utils import ndiffs\n",
    "## Adf Test\n",
    "print(ndiffs(y_train, test='adf'))  # 1\n",
    "\n",
    "# KPSS test\n",
    "print(ndiffs(y_train, test='kpss'))  # 1\n",
    "\n",
    "# PP test:\n",
    "print(ndiffs(y_train, test='pp'))  # 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "36946e35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               SARIMAX Results                                \n",
      "==============================================================================\n",
      "Dep. Variable:                      0   No. Observations:                23325\n",
      "Model:                 ARIMA(1, 1, 1)   Log Likelihood              116740.996\n",
      "Date:                Thu, 14 Apr 2022   AIC                        -233475.993\n",
      "Time:                        22:04:42   BIC                        -233451.821\n",
      "Sample:                             0   HQIC                       -233468.143\n",
      "                              - 23325                                         \n",
      "Covariance Type:                  opg                                         \n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "ar.L1          0.0446      0.020      2.216      0.027       0.005       0.084\n",
      "ma.L1          0.0551      0.021      2.688      0.007       0.015       0.095\n",
      "sigma2      2.618e-06   3.59e-09    729.886      0.000    2.61e-06    2.63e-06\n",
      "===================================================================================\n",
      "Ljung-Box (L1) (Q):                   0.00   Jarque-Bera (JB):          17011629.74\n",
      "Prob(Q):                              0.96   Prob(JB):                         0.00\n",
      "Heteroskedasticity (H):               0.95   Skew:                             2.65\n",
      "Prob(H) (two-sided):                  0.04   Kurtosis:                       135.20\n",
      "===================================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Covariance matrix calculated using the outer product of gradients (complex-step).\n"
     ]
    }
   ],
   "source": [
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "model = ARIMA(y_train, order=(1,1,1))\n",
    "model_fit = model.fit()\n",
    "print(model_fit.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0dbcc69b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23325    0.005246\n",
      "23326    0.005260\n",
      "23327    0.005261\n",
      "Name: predicted_mean, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "pred = model_fit.forecast(steps=3)\n",
    "print(pred)\n",
    "#np.corrcoef(pred, y_train.iloc[:,0])[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "fdb3f0ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23324</th>\n",
       "      <td>0.004917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23325</th>\n",
       "      <td>0.006584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23326</th>\n",
       "      <td>0.007966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23327</th>\n",
       "      <td>0.009550</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              0\n",
       "23324  0.004917\n",
       "23325  0.006584\n",
       "23326  0.007966\n",
       "23327  0.009550"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.iloc[23324:23325+3,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f56ecfa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shushuz/opt/anaconda3/lib/python3.8/site-packages/statsmodels/tsa/statespace/sarimax.py:966: UserWarning: Non-stationary starting autoregressive parameters found. Using zeros as starting parameters.\n",
      "  warn('Non-stationary starting autoregressive parameters'\n",
      "/Users/shushuz/opt/anaconda3/lib/python3.8/site-packages/statsmodels/tsa/statespace/sarimax.py:978: UserWarning: Non-invertible starting MA parameters found. Using zeros as starting parameters.\n",
      "  warn('Non-invertible starting MA parameters found.'\n",
      "/Users/shushuz/opt/anaconda3/lib/python3.8/site-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4519243724646293"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ARIMA(y.iloc[23325-144:23325,:], order=(1,1,1))\n",
    "model_fit = model.fit()\n",
    "pred = model_fit.predict(step=3)\n",
    "np.corrcoef(pred, y.iloc[23325+3-144:23325+3,0])[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "5a7dfd19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct data\n",
    "for asset in range(1,10):\n",
    "#    t0 = time.time()\n",
    "\n",
    "    fs = get_features(log_pr.iloc[14400:(1440 * 11), asset], volu.iloc[14400:(1440 * 11), asset])\n",
    "    y = np.mean(log_pr.iloc[(1440 * 11+27):(1440 * 11+33), asset])\n",
    "\n",
    "    d = 10\n",
    "\n",
    "    for t in range(1440 * 0, 264960 - 1470, d): \n",
    "        f = get_features(log_pr.iloc[t:(t+1440), asset], volu.iloc[t:(t+1440), asset])\n",
    "        fs = np.vstack((fs, f))\n",
    "        y = np.vstack((\n",
    "            y, \n",
    "            np.mean(log_pr.iloc[(t+1440+27):(t+1440+33), asset])\n",
    "        ))\n",
    "    pd.DataFrame(fs).to_pickle(\"feature\"+str(asset)+\".df\")\n",
    "    pd.DataFrame(y).to_pickle(\"y\"+str(asset)+\".df\")\n",
    "\n",
    "#     Y_norm_train = pd.DataFrame(mystandardize(y[:,0])[0])\n",
    "#     model = ElasticNetCV(alphas=np.linspace(1000, 5e5, num=100), l1_ratio = 1,\n",
    "#                               fit_intercept = True, normalize = False, tol=0.0000001, max_iter = 100000)\n",
    "#     model.fit(fs, Y_norm_train)\n",
    "\n",
    "#     with open('./model_{}.pkl'.format(asset),'wb') as f:\n",
    "#         pickle.dump(model,f)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60cb4617",
   "metadata": {},
   "source": [
    "# Train Model for Submission Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0519ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the models for submission with Cluster\n",
    "\n",
    "### group 1\n",
    "for asset in [0,3,5,6]:\n",
    "    fs = pd.read_pickle(\"feature\"+str(asset)+\".df\")\n",
    "    y = pd.read_pickle(\"y\"+str(asset)+\".df\")\n",
    "\n",
    "    Y_norm_train = pd.DataFrame(mystandardize(y)[0])\n",
    "    model = ElasticNetCV(l1_ratio = 1,\n",
    "                         #alphas=np.linspace(1000, 5e5, num=100), \n",
    "                              fit_intercept = True, normalize = False, \n",
    "                         tol=0.0000001, max_iter = 100000)\n",
    "    model.fit(fs, Y_norm_train)\n",
    "\n",
    "    with open('./model_{}.pkl'.format(asset),'wb') as f:\n",
    "        pickle.dump(model,f)\n",
    "        \n",
    "        \n",
    "### group 2\n",
    "for asset in [1,4,7]:\n",
    "    fs = pd.read_pickle(\"feature\"+str(asset)+\".df\")\n",
    "    y = pd.read_pickle(\"y\"+str(asset)+\".df\")\n",
    "\n",
    "    Y_norm_train = pd.DataFrame(mystandardize(y)[0])\n",
    "    model = ElasticNetCV(l1_ratio = 1,\n",
    "                         alphas=np.linspace(10, 100, num=100), \n",
    "                              fit_intercept = True, normalize = False, \n",
    "                         tol=0.0000001, max_iter = 100000)\n",
    "    model.fit(fs, Y_norm_train)\n",
    "\n",
    "    with open('./model_{}.pkl'.format(asset),'wb') as f:\n",
    "        pickle.dump(model,f)\n",
    "        \n",
    "### group 3\n",
    "for asset in [2]:\n",
    "    fs = pd.read_pickle(\"feature\"+str(asset)+\".df\")\n",
    "    y = pd.read_pickle(\"y\"+str(asset)+\".df\")\n",
    "\n",
    "    Y_norm_train = pd.DataFrame(mystandardize(y)[0])\n",
    "    model = ElasticNetCV(l1_ratio = 0.5,\n",
    "                         alphas=np.linspace(1000, 5e5, num=100), \n",
    "                              fit_intercept = True, normalize = False, \n",
    "                         tol=0.0000001, max_iter = 100000)\n",
    "    model.fit(fs, Y_norm_train)\n",
    "\n",
    "    with open('./model_{}.pkl'.format(asset),'wb') as f:\n",
    "        pickle.dump(model,f)\n",
    "\n",
    "### group 4\n",
    "for asset in [8,9]:\n",
    "    fs = pd.read_pickle(\"feature\"+str(asset)+\".df\")\n",
    "    y = pd.read_pickle(\"y\"+str(asset)+\".df\")\n",
    "\n",
    "    Y_norm_train = pd.DataFrame(mystandardize(y)[0])\n",
    "    model = ElasticNetCV(l1_ratio = 0.1,\n",
    "                         alphas=np.linspace(100, 1000, num=100), \n",
    "                              fit_intercept = True, normalize = False, \n",
    "                         tol=0.0000001, max_iter = 100000)\n",
    "    model.fit(fs, Y_norm_train)\n",
    "\n",
    "    with open('./model_{}.pkl'.format(asset),'wb') as f:\n",
    "        pickle.dump(model,f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "ec633f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODELS = []\n",
    "\n",
    "# for i in range(0,10):\n",
    "#     with open('model_{}.pkl'.format(i), 'rb') as f:\n",
    "#         model = pickle.load(f)\n",
    "#         MODELS.append(model)\n",
    "        \n",
    "def get_r_hat(A, B): \n",
    "    \"\"\"\n",
    "        A: 1440-by-10 dataframe of log prices with columns log_pr_0, ... , log_pr_9\n",
    "        B: 1440-by-10 dataframe of trading volumes with columns volu_0, ... , volu_9    \n",
    "        return: a numpy array of length 10, \n",
    "            corresponding to the predictions for the forward 30-minutes returns of assets 0, 1, 2, ..., 9\n",
    "    \"\"\"\n",
    "    answer = []\n",
    "    df = A.iloc[29::30,:]\n",
    "    # asset 0\n",
    "    asset = 0\n",
    "    model = ARIMA(df[asset], order=(1,2,1))\n",
    "    model_fit = model.fit()\n",
    "    pred = model_fit.forecast(steps=1) - A.iloc[-1, asset]\n",
    "    answer.append(pred)\n",
    "        \n",
    "    for asset in range(1,10):\n",
    "        \n",
    "#         f = get_features(np.array(A)[:, asset], np.array(B)[:, asset])\n",
    "        \n",
    "#         pred = MODELS[asset].predict(f).reshape(-1, 1)- A.iloc[-1, asset]\n",
    "        model = ARIMA(df[asset], order=(1,1,1))\n",
    "        model_fit = model.fit()\n",
    "        pred = model_fit.forecast(steps=1) - A.iloc[-1, asset]\n",
    "        answer.append(pred)\n",
    "    \n",
    "    answer = np.array(answer).reshape(10)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "1bd67024",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_r_hat(A, B): \n",
    "    \"\"\"\n",
    "        A: 1440-by-10 dataframe of log prices with columns log_pr_0, ... , log_pr_9\n",
    "        B: 1440-by-10 dataframe of trading volumes with columns volu_0, ... , volu_9    \n",
    "        return: a numpy array of length 10, \n",
    "            corresponding to the predictions for the forward 30-minutes returns of assets 0, 1, 2, ..., 9\n",
    "    \"\"\"\n",
    "    answer = []\n",
    "    # asset 0\n",
    "    asset = 0\n",
    "    model = ARIMA(A[asset], order=(1,2,1))\n",
    "    model_fit = model.fit()\n",
    "    pred = model_fit.forecast(steps=30)[-1] - A.iloc[-1, asset]\n",
    "    print(model_fit.forecast(steps=30)[-1])\n",
    "    answer.append(pred)\n",
    "        \n",
    "    for asset in range(1,10):\n",
    "        \n",
    "#         f = get_features(np.array(A)[:, asset], np.array(B)[:, asset])\n",
    "        \n",
    "#         pred = MODELS[asset].predict(f).reshape(-1, 1)- A.iloc[-1, asset]\n",
    "        model = ARIMA(A[asset], order=(1,1,1))\n",
    "        model_fit = model.fit()\n",
    "        pred = model_fit.forecast(steps=30)[-1] - A.iloc[-1, asset]\n",
    "        answer.append(pred)\n",
    "    \n",
    "    answer = np.array(answer).reshape(10)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "5c217d3b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shushuz/opt/anaconda3/lib/python3.8/site-packages/statsmodels/tsa/base/tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency T will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "/Users/shushuz/opt/anaconda3/lib/python3.8/site-packages/statsmodels/tsa/base/tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency T will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "/Users/shushuz/opt/anaconda3/lib/python3.8/site-packages/statsmodels/tsa/base/tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency T will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "/Users/shushuz/opt/anaconda3/lib/python3.8/site-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "/Users/shushuz/opt/anaconda3/lib/python3.8/site-packages/statsmodels/tsa/base/tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency T will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "/Users/shushuz/opt/anaconda3/lib/python3.8/site-packages/statsmodels/tsa/base/tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency T will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "/Users/shushuz/opt/anaconda3/lib/python3.8/site-packages/statsmodels/tsa/base/tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency T will be used.\n",
      "  self._init_dates(dates, freq)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.00503879349113658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shushuz/opt/anaconda3/lib/python3.8/site-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "/Users/shushuz/opt/anaconda3/lib/python3.8/site-packages/statsmodels/tsa/base/tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency T will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "/Users/shushuz/opt/anaconda3/lib/python3.8/site-packages/statsmodels/tsa/base/tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency T will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "/Users/shushuz/opt/anaconda3/lib/python3.8/site-packages/statsmodels/tsa/base/tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency T will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "/Users/shushuz/opt/anaconda3/lib/python3.8/site-packages/statsmodels/tsa/statespace/sarimax.py:966: UserWarning: Non-stationary starting autoregressive parameters found. Using zeros as starting parameters.\n",
      "  warn('Non-stationary starting autoregressive parameters'\n",
      "/Users/shushuz/opt/anaconda3/lib/python3.8/site-packages/statsmodels/tsa/statespace/sarimax.py:978: UserWarning: Non-invertible starting MA parameters found. Using zeros as starting parameters.\n",
      "  warn('Non-invertible starting MA parameters found.'\n",
      "/Users/shushuz/opt/anaconda3/lib/python3.8/site-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "/Users/shushuz/opt/anaconda3/lib/python3.8/site-packages/statsmodels/tsa/base/tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency T will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "/Users/shushuz/opt/anaconda3/lib/python3.8/site-packages/statsmodels/tsa/base/tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency T will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "/Users/shushuz/opt/anaconda3/lib/python3.8/site-packages/statsmodels/tsa/base/tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency T will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "/Users/shushuz/opt/anaconda3/lib/python3.8/site-packages/statsmodels/tsa/base/tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency T will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "/Users/shushuz/opt/anaconda3/lib/python3.8/site-packages/statsmodels/tsa/base/tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency T will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "/Users/shushuz/opt/anaconda3/lib/python3.8/site-packages/statsmodels/tsa/base/tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency T will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "/Users/shushuz/opt/anaconda3/lib/python3.8/site-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "/Users/shushuz/opt/anaconda3/lib/python3.8/site-packages/statsmodels/tsa/base/tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency T will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "/Users/shushuz/opt/anaconda3/lib/python3.8/site-packages/statsmodels/tsa/base/tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency T will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "/Users/shushuz/opt/anaconda3/lib/python3.8/site-packages/statsmodels/tsa/base/tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency T will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "/Users/shushuz/opt/anaconda3/lib/python3.8/site-packages/statsmodels/tsa/base/tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency T will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "/Users/shushuz/opt/anaconda3/lib/python3.8/site-packages/statsmodels/tsa/base/tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency T will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "/Users/shushuz/opt/anaconda3/lib/python3.8/site-packages/statsmodels/tsa/base/tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency T will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "/Users/shushuz/opt/anaconda3/lib/python3.8/site-packages/statsmodels/tsa/base/tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency T will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "/Users/shushuz/opt/anaconda3/lib/python3.8/site-packages/statsmodels/tsa/base/tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency T will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "/Users/shushuz/opt/anaconda3/lib/python3.8/site-packages/statsmodels/tsa/base/tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency T will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "/Users/shushuz/opt/anaconda3/lib/python3.8/site-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "/Users/shushuz/opt/anaconda3/lib/python3.8/site-packages/statsmodels/tsa/base/tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency T will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "/Users/shushuz/opt/anaconda3/lib/python3.8/site-packages/statsmodels/tsa/base/tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency T will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "/Users/shushuz/opt/anaconda3/lib/python3.8/site-packages/statsmodels/tsa/base/tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency T will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "/Users/shushuz/opt/anaconda3/lib/python3.8/site-packages/statsmodels/tsa/base/tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency T will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "/Users/shushuz/opt/anaconda3/lib/python3.8/site-packages/statsmodels/tsa/base/tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency T will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "/Users/shushuz/opt/anaconda3/lib/python3.8/site-packages/statsmodels/tsa/base/tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency T will be used.\n",
      "  self._init_dates(dates, freq)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 1.        , -0.03668557],\n",
       "       [-0.03668557,  1.        ]])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.corrcoef(get_r_hat(log_pr.iloc[1440:1440*2,:],volu.iloc[1440:1440*2,:]),log_pr.iloc[1440*2+30,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "0f387ccc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "timestamp\n",
       "2021-07-01 00:29:00   -0.001411\n",
       "2021-07-01 00:59:00   -0.009531\n",
       "2021-07-01 01:29:00   -0.012284\n",
       "2021-07-01 01:59:00   -0.008864\n",
       "2021-07-01 02:29:00   -0.008747\n",
       "2021-07-01 02:59:00   -0.010647\n",
       "2021-07-01 03:29:00   -0.004762\n",
       "2021-07-01 03:59:00    0.001056\n",
       "2021-07-01 04:29:00    0.001116\n",
       "2021-07-01 04:59:00   -0.000264\n",
       "2021-07-01 05:29:00   -0.002318\n",
       "2021-07-01 05:59:00   -0.001721\n",
       "2021-07-01 06:29:00    0.001954\n",
       "2021-07-01 06:59:00    0.004308\n",
       "2021-07-01 07:29:00    0.004084\n",
       "2021-07-01 07:59:00    0.007866\n",
       "2021-07-01 08:29:00   -0.000171\n",
       "2021-07-01 08:59:00   -0.003883\n",
       "2021-07-01 09:29:00   -0.000983\n",
       "2021-07-01 09:59:00   -0.001637\n",
       "2021-07-01 10:29:00    0.001903\n",
       "2021-07-01 10:59:00    0.002995\n",
       "2021-07-01 11:29:00    0.000836\n",
       "2021-07-01 11:59:00    0.004060\n",
       "2021-07-01 12:29:00    0.003237\n",
       "2021-07-01 12:59:00    0.001190\n",
       "2021-07-01 13:29:00    0.000952\n",
       "2021-07-01 13:59:00    0.001787\n",
       "2021-07-01 14:29:00    0.001455\n",
       "2021-07-01 14:59:00    0.003093\n",
       "2021-07-01 15:29:00   -0.001615\n",
       "2021-07-01 15:59:00    0.002341\n",
       "2021-07-01 16:29:00    0.004370\n",
       "2021-07-01 16:59:00    0.006960\n",
       "2021-07-01 17:29:00    0.005342\n",
       "2021-07-01 17:59:00    0.001678\n",
       "2021-07-01 18:29:00   -0.001167\n",
       "2021-07-01 18:59:00   -0.001814\n",
       "2021-07-01 19:29:00   -0.000648\n",
       "2021-07-01 19:59:00    0.000055\n",
       "2021-07-01 20:29:00   -0.005093\n",
       "2021-07-01 20:59:00   -0.005060\n",
       "2021-07-01 21:29:00   -0.009639\n",
       "2021-07-01 21:59:00   -0.009003\n",
       "2021-07-01 22:29:00   -0.010876\n",
       "2021-07-01 22:59:00   -0.011719\n",
       "2021-07-01 23:29:00   -0.011572\n",
       "2021-07-01 23:59:00   -0.013186\n",
       "Name: 0, dtype: float64"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = log_pr.iloc[:1440,:]\n",
    "A.iloc[29::30,:][0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
